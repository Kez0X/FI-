# Le√ßon : Ind√©pendance des variables al√©atoires üîÑ

## Objectifs üéØ :
- Comprendre la notion d'ind√©pendance entre deux variables al√©atoires.
- Apprendre √† d√©terminer si deux variables al√©atoires sont ind√©pendantes.
- Savoir utiliser l'ind√©pendance dans les calculs de probabilit√©s.



## 1. D√©finition de l'ind√©pendance des variables al√©atoires üîç

Deux variables al√©atoires $$ X $$ et $$ Y $$ sont dites **ind√©pendantes** si la r√©alisation de l'une n'influence en rien la r√©alisation de l'autre. Autrement dit, la probabilit√© de r√©aliser une certaine combinaison de valeurs pour $$ X $$ et $$ Y $$ est simplement le produit des probabilit√©s de r√©aliser ces valeurs s√©par√©ment.

### 1.1. Formulation de l'ind√©pendance
Deux variables al√©atoires $$ X $$ et $$ Y $$ sont ind√©pendantes si et seulement si, pour toutes les valeurs possibles $$ x $$ et $$ y $$ que peuvent prendre $$ X $$ et $$ Y $$, on a la relation suivante :

$$
P(X = x \text{ et } Y = y) = P(X = x) \cdot P(Y = y)
$$

Cela signifie que la probabilit√© que $$ X $$ prenne la valeur $$ x $$ et que $$ Y $$ prenne la valeur $$ y $$ simultan√©ment est √©gale au produit des probabilit√©s individuelles de $$ X = x $$ et $$ Y = y $$.

### 1.2. Ind√©pendance conditionnelle
On dit que deux variables al√©atoires $$ X $$ et $$ Y $$ sont ind√©pendantes **conditionnellement** √† une troisi√®me variable $$ Z $$ si :

$$
P(X = x \text{ et } Y = y \mid Z = z) = P(X = x \mid Z = z) \cdot P(Y = y \mid Z = z)
$$

Cela signifie que, sachant que $$ Z $$ a pris une certaine valeur, l'ind√©pendance de $$ X $$ et $$ Y $$ est pr√©serv√©e.



## 2. V√©rification de l'ind√©pendance des variables al√©atoires üìä

### 2.1. Pour des variables al√©atoires discr√®tes
Si $$ X $$ et $$ Y $$ sont discr√®tes, on v√©rifie l'ind√©pendance en comparant la probabilit√© conjointe des deux variables avec le produit des probabilit√©s individuelles. Autrement dit, on v√©rifie si la condition suivante est satisfaite pour toutes les valeurs possibles de $$ X $$ et $$ Y $$ :

$$
P(X = x \text{ et } Y = y) = P(X = x) \cdot P(Y = y)
$$

### 2.2. Exemple d'ind√©pendance discr√®te
Soit $$ X $$ une variable al√©atoire repr√©sentant le r√©sultat d'un lancer de d√© (avec $$ P(X = 1) = P(X = 2) = \dots = P(X = 6) = \frac{1}{6} $$), et soit $$ Y $$ une autre variable al√©atoire repr√©sentant le r√©sultat d'un lancer d'une pi√®ce (avec $$ P(Y = 0) = P(Y = 1) = \frac{1}{2} $$).

On v√©rifie l'ind√©pendance de $$ X $$ et $$ Y $$ en comparant la probabilit√© conjointe avec le produit des probabilit√©s :

$$
P(X = x \text{ et } Y = y) = P(X = x) \cdot P(Y = y)
$$

Par exemple, si $$ X = 1 $$ et $$ Y = 0 $$ :

$$
P(X = 1 \text{ et } Y = 0) = \frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}
$$

Si cette √©galit√© est v√©rifi√©e pour toutes les valeurs possibles de $$ X $$ et $$ Y $$, alors $$ X $$ et $$ Y $$ sont ind√©pendantes.

### 2.3. Pour des variables al√©atoires continues
Si $$ X $$ et $$ Y $$ sont continues, on v√©rifie l'ind√©pendance en utilisant les densit√©s de probabilit√© respectives. Les variables sont ind√©pendantes si et seulement si la fonction de densit√© conjointe de $$ X $$ et $$ Y $$ peut √™tre factoris√©e sous la forme du produit des fonctions de densit√© individuelles :

$$
f_{X,Y}(x, y) = f_X(x) \cdot f_Y(y)
$$



## 3. Propri√©t√©s de l'ind√©pendance ‚ú®

### 3.1. L'ind√©pendance implique des relations sur les moments
Si $$ X $$ et $$ Y $$ sont ind√©pendantes, on peut exprimer leur esp√©rance conjointe de la mani√®re suivante :

$$
E(XY) = E(X) \cdot E(Y)
$$

De m√™me, pour les variances, on a :

$$
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)
$$

Cela d√©coule du fait que l'ind√©pendance permet de d√©composer les moments d'ordre sup√©rieur en produits de moments individuels.

### 3.2. Ind√©pendance et probabilit√© conditionnelle
Si $$ X $$ et $$ Y $$ sont ind√©pendantes, cela signifie que la connaissance de $$ X $$ n'apporte aucune information suppl√©mentaire sur $$ Y $$, et vice versa. Par cons√©quent, on a :

$$
P(Y = y \mid X = x) = P(Y = y)
$$

Cela vaut aussi bien pour les variables discr√®tes que continues.



## 4. Applications de l'ind√©pendance üßë‚Äçüè´

### 4.1. Exemple pratique : Lancer d'un d√© et d'une pi√®ce
Consid√©rons un jeu o√π un d√© est lanc√© et une pi√®ce est lanc√©e. Soit $$ X $$ la variable al√©atoire repr√©sentant le r√©sultat du lancer de d√© ($$ X \in \{1, 2, 3, 4, 5, 6\} $$), et soit $$ Y $$ la variable al√©atoire repr√©sentant le r√©sultat du lancer de la pi√®ce ($$ Y \in \{0, 1\} $$, o√π $$ 0 $$ repr√©sente pile et $$ 1 $$ repr√©sente face).

On suppose que les deux √©v√©nements sont ind√©pendants. Ainsi, la probabilit√© de l'√©v√©nement $$ (X = 3 \text{ et } Y = 1) $$ est donn√©e par :

$$
P(X = 3 \text{ et } Y = 1) = P(X = 3) \cdot P(Y = 1) = \frac{1}{6} \cdot \frac{1}{2} = \frac{1}{12}
$$

### 4.2. Exemple d'ind√©pendance conditionnelle
Supposons qu'une variable al√©atoire $$ X $$ repr√©sente la temp√©rature dans une ville et une variable al√©atoire $$ Y $$ repr√©sente la probabilit√© de pluie. Si on conna√Æt les pr√©visions m√©t√©orologiques pour un jour donn√©, alors les √©v√©nements relatifs √† $$ X $$ et $$ Y $$ peuvent √™tre consid√©r√©s comme ind√©pendants conditionnellement √† ces pr√©visions.



## 5. Conclusion üåü

L'ind√©pendance des variables al√©atoires est une notion fondamentale en th√©orie des probabilit√©s. Elle simplifie grandement les calculs de probabilit√©s, car elle permet de factoriser les probabilit√©s conjointes. Comprendre et identifier des variables ind√©pendantes est crucial dans de nombreux domaines, comme les statistiques, l'√©conomie, et les sciences sociales.

L'ind√©pendance conditionnelle permet √©galement de traiter des situations complexes o√π les √©v√©nements sont ind√©pendants sous certaines conditions.



## Devoirs üìë

1. V√©rifier si deux variables al√©atoires discr√®tes $$ X $$ et $$ Y $$, avec les lois de probabilit√© suivantes, sont ind√©pendantes :
   - $$ P(X = 1) = 0.3, P(X = 2) = 0.7 $$.
   - $$ P(Y = 1) = 0.4, P(Y = 2) = 0.6 $$.
   - $$ P(X = 1 \text{ et } Y = 1) = 0.12, P(X = 2 \text{ et } Y = 2) = 0.42 $$.

2. Montrer que deux variables al√©atoires ind√©pendantes respectent la propri√©t√© $$ E(XY) = E(X) \cdot E(Y) $$.

3. R√©soudre un probl√®me dans lequel on doit d√©terminer si deux √©v√©nements sont ind√©pendants dans un contexte de probabilit√© conditionnelle.



